{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM_Autoencoders_with_Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Autoencoders ?\n",
    "An **Autoencoder** is a type of Artificial Neural Network model that learns a compressed representation of input (i.e., [Wiki](https://en.wikipedia.org/wiki/Autoencoder)). LSTM autoencoder is an implementation for compressed sequence representation for Encoder-Decoder LSTM. The Encoder part can compress the original input sequence to a fixed length, which can be used as feature vector for other supervised learning algorithms or even data visualization.\n",
    "\n",
    "The lenght of sequences in any sequence prediction problems might vary from time to time. But, ANN are usaually designed to work with fixed length feature vectors. Hence, encoders play important roles in such scenarios, where the they encode the sequences of varying length to a fixed length reprsentation, making them suitable for the input of ANN models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Encoder-Decoder LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0],\n",
       "        [ 1],\n",
       "        [ 2],\n",
       "        [ 3],\n",
       "        [ 4],\n",
       "        [ 5],\n",
       "        [ 6],\n",
       "        [ 7],\n",
       "        [ 8],\n",
       "        [ 9],\n",
       "        [10],\n",
       "        [11],\n",
       "        [12],\n",
       "        [13],\n",
       "        [14],\n",
       "        [15],\n",
       "        [16],\n",
       "        [17],\n",
       "        [18],\n",
       "        [19],\n",
       "        [20],\n",
       "        [21],\n",
       "        [22],\n",
       "        [23],\n",
       "        [24],\n",
       "        [25],\n",
       "        [26],\n",
       "        [27],\n",
       "        [28],\n",
       "        [29],\n",
       "        [30],\n",
       "        [31],\n",
       "        [32],\n",
       "        [33],\n",
       "        [34],\n",
       "        [35]]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import array\n",
    "\n",
    "np.arange(36).reshape(1, 36, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE: \n",
    "The shape (x,y,z) means x number of arrays containing y rows by z columns. That is (1, 36, 1) means a column vector (1 column) with 36 rows and there are 1 of such vectors, as displayed above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Reconstruction: Encoder-Decoder LSTM\n",
    "This is the simplest LSTM autoencoder, that just learns to reconstruct the input sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 The Input Sequence to Reconstruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define input sequence\n",
    "sequence = array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])\n",
    "print(type(sequence))\n",
    "# reshape input into [samples, timesteps, features]\n",
    "n_in = len(sequence)\n",
    "sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Reshape for LSTM\n",
    "Keras LSTM always expect 3D array as input. The 3D tensor with shape defines **(batch_size, timesteps, input_dim)**. Here, for this simple example we only use single batch. The number of timesteps are 9 (i.e., 0.1 through 0.9). And per input or feature dimensions per timesteps is only 1 (i.e., single number 0.1 or 0.2 and so on). Hence we reshape input as per the required for LSTM input, i.e., (1, 9, 1), ie., a **3D array as presented in the following:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.1],\n",
       "        [0.2],\n",
       "        [0.3],\n",
       "        [0.4],\n",
       "        [0.5],\n",
       "        [0.6],\n",
       "        [0.7],\n",
       "        [0.8],\n",
       "        [0.9]]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence = sequence.reshape((1, n_in, 1))\n",
    "sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Define Model\n",
    "\n",
    "#### Useful Link:: Understanding LSTM setup in Keras\n",
    "1. https://stackoverflow.com/questions/38714959/understanding-keras-lstms#\n",
    "2. https://medium.com/@shivajbd/understanding-input-and-output-shape-in-lstm-keras-c501ee95c65e\n",
    "3. (Keep Standalone LSTM Encoder) https://machinelearningmastery.com/lstm-autoencoders/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               40800     \n",
      "_________________________________________________________________\n",
      "repeat_vector_1 (RepeatVecto (None, 9, 100)            0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 9, 100)            80400     \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 9, 1)              101       \n",
      "=================================================================\n",
      "Total params: 121,301\n",
      "Trainable params: 121,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Model\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, activation='relu', input_shape=(n_in,1)))\n",
    "model.add(RepeatVector(n_in))\n",
    "model.add(LSTM(100, activation='relu', return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(1)))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Note: Understanding the Model shapes and Structures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.1 Layer 01 (LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`model.add(LSTM(100, activation='relu', input_shape=(n_in,1)))`\n",
    "\n",
    "**>>Input:** The first step of the model takes the 3D array as input as reshaped above. We do not have any batch for this example, so **None** and we do not need to specify it, i.e., `input_shape=(n_in,1)` is equivalent to `input_shape=(None,n_in,1)`. That is, **batch = Unkown/None, timesteps = 9, and feature dimension per time step 1**. This first LSTM layer contains 100 nodes.\n",
    "\n",
    "**>>Output:** The output shape is (None, 100). We can use the following sample code to examine the output by layers. For example, the ouput of the first layer can be found as following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 100)\n",
      "[[0.         0.08526425 0.         0.08668269 0.         0.\n",
      "  0.         0.08373296 0.06473152 0.         0.10590955 0.13052945\n",
      "  0.00632676 0.09575267 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.09235387 0.         0.03934644\n",
      "  0.03931042 0.         0.05035766 0.         0.         0.\n",
      "  0.         0.0008235  0.         0.         0.10211127 0.10406243\n",
      "  0.         0.         0.         0.10208047 0.02607506 0.\n",
      "  0.04599658 0.06028002 0.         0.06495491 0.04745879 0.06214409\n",
      "  0.         0.         0.         0.09650227 0.05514612 0.\n",
      "  0.         0.04339869 0.0507921  0.10846753 0.         0.\n",
      "  0.02928454 0.         0.03709463 0.06268398 0.11128254 0.\n",
      "  0.08007399 0.         0.         0.         0.         0.0214268\n",
      "  0.         0.         0.05982703 0.06501038 0.06495484 0.08794844\n",
      "  0.08229375 0.         0.         0.00263113 0.10752204 0.\n",
      "  0.         0.         0.10252966 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.05054668\n",
      "  0.         0.         0.06750368 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# lstm autoencoder recreate sequence\n",
    "from numpy import array\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.utils import plot_model\n",
    "\n",
    "sequence = array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])\n",
    "\n",
    "n_in = len(sequence)\n",
    "sequence = sequence.reshape((1, n_in, 1))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, activation='relu', input_shape=(n_in,1)))\n",
    "model.add(RepeatVector(n_in))\n",
    "model.add(LSTM(100, activation='relu', return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(1)))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "# fit model\n",
    "model.fit(sequence, sequence, epochs=300, verbose=0)\n",
    "\n",
    "################################\n",
    "# Output of Layer 0\n",
    "################################\n",
    "model = Model(inputs=model.inputs, outputs=model.layers[0].output)\n",
    "\n",
    "# get the feature vector for the input sequence\n",
    "yhat = model.predict(sequence)\n",
    "print(yhat.shape)\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.2 Layer 02 (RepeatVector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can note from the above, the output from the previous layer is a 2D array, ie., (1, 100). However, as the next layer is also another LSTM, it accepts only 3D array as discussed above. We hence use this layer, i.e., **RepeatVector** to simply duplicate the output vector to make it suitable as input for the next LSTM. Also, note that is nothing to learn by the network at this step, hence the **Param #**  in model summary is 0. The outpu of this RepeatVector layer is as following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 9, 100)\n",
      "[[[0.         0.         0.06829739 0.09849601 0.11844321 0.\n",
      "   0.10648596 0.         0.         0.06946453 0.         0.00135783\n",
      "   0.04175166 0.         0.         0.10055738 0.10648762 0.\n",
      "   0.         0.         0.         0.07734827 0.         0.\n",
      "   0.         0.07906882 0.         0.         0.         0.02494789\n",
      "   0.         0.09046537 0.05006797 0.         0.02365715 0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.13598846 0.         0.05468839 0.04727116 0.         0.\n",
      "   0.         0.03248655 0.05415042 0.         0.         0.\n",
      "   0.         0.00435075 0.09720369 0.         0.         0.\n",
      "   0.08369314 0.         0.         0.05793652 0.         0.08219028\n",
      "   0.         0.         0.01177892 0.10881354 0.         0.\n",
      "   0.         0.06474225 0.05809931 0.         0.         0.02664913\n",
      "   0.01288957 0.         0.03107853 0.         0.         0.\n",
      "   0.13248082 0.         0.         0.         0.         0.\n",
      "   0.         0.10522534 0.         0.07495283 0.03101982 0.05283207\n",
      "   0.00466938 0.09514671 0.00054616 0.        ]\n",
      "  [0.         0.         0.06829739 0.09849601 0.11844321 0.\n",
      "   0.10648596 0.         0.         0.06946453 0.         0.00135783\n",
      "   0.04175166 0.         0.         0.10055738 0.10648762 0.\n",
      "   0.         0.         0.         0.07734827 0.         0.\n",
      "   0.         0.07906882 0.         0.         0.         0.02494789\n",
      "   0.         0.09046537 0.05006797 0.         0.02365715 0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.13598846 0.         0.05468839 0.04727116 0.         0.\n",
      "   0.         0.03248655 0.05415042 0.         0.         0.\n",
      "   0.         0.00435075 0.09720369 0.         0.         0.\n",
      "   0.08369314 0.         0.         0.05793652 0.         0.08219028\n",
      "   0.         0.         0.01177892 0.10881354 0.         0.\n",
      "   0.         0.06474225 0.05809931 0.         0.         0.02664913\n",
      "   0.01288957 0.         0.03107853 0.         0.         0.\n",
      "   0.13248082 0.         0.         0.         0.         0.\n",
      "   0.         0.10522534 0.         0.07495283 0.03101982 0.05283207\n",
      "   0.00466938 0.09514671 0.00054616 0.        ]\n",
      "  [0.         0.         0.06829739 0.09849601 0.11844321 0.\n",
      "   0.10648596 0.         0.         0.06946453 0.         0.00135783\n",
      "   0.04175166 0.         0.         0.10055738 0.10648762 0.\n",
      "   0.         0.         0.         0.07734827 0.         0.\n",
      "   0.         0.07906882 0.         0.         0.         0.02494789\n",
      "   0.         0.09046537 0.05006797 0.         0.02365715 0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.13598846 0.         0.05468839 0.04727116 0.         0.\n",
      "   0.         0.03248655 0.05415042 0.         0.         0.\n",
      "   0.         0.00435075 0.09720369 0.         0.         0.\n",
      "   0.08369314 0.         0.         0.05793652 0.         0.08219028\n",
      "   0.         0.         0.01177892 0.10881354 0.         0.\n",
      "   0.         0.06474225 0.05809931 0.         0.         0.02664913\n",
      "   0.01288957 0.         0.03107853 0.         0.         0.\n",
      "   0.13248082 0.         0.         0.         0.         0.\n",
      "   0.         0.10522534 0.         0.07495283 0.03101982 0.05283207\n",
      "   0.00466938 0.09514671 0.00054616 0.        ]\n",
      "  [0.         0.         0.06829739 0.09849601 0.11844321 0.\n",
      "   0.10648596 0.         0.         0.06946453 0.         0.00135783\n",
      "   0.04175166 0.         0.         0.10055738 0.10648762 0.\n",
      "   0.         0.         0.         0.07734827 0.         0.\n",
      "   0.         0.07906882 0.         0.         0.         0.02494789\n",
      "   0.         0.09046537 0.05006797 0.         0.02365715 0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.13598846 0.         0.05468839 0.04727116 0.         0.\n",
      "   0.         0.03248655 0.05415042 0.         0.         0.\n",
      "   0.         0.00435075 0.09720369 0.         0.         0.\n",
      "   0.08369314 0.         0.         0.05793652 0.         0.08219028\n",
      "   0.         0.         0.01177892 0.10881354 0.         0.\n",
      "   0.         0.06474225 0.05809931 0.         0.         0.02664913\n",
      "   0.01288957 0.         0.03107853 0.         0.         0.\n",
      "   0.13248082 0.         0.         0.         0.         0.\n",
      "   0.         0.10522534 0.         0.07495283 0.03101982 0.05283207\n",
      "   0.00466938 0.09514671 0.00054616 0.        ]\n",
      "  [0.         0.         0.06829739 0.09849601 0.11844321 0.\n",
      "   0.10648596 0.         0.         0.06946453 0.         0.00135783\n",
      "   0.04175166 0.         0.         0.10055738 0.10648762 0.\n",
      "   0.         0.         0.         0.07734827 0.         0.\n",
      "   0.         0.07906882 0.         0.         0.         0.02494789\n",
      "   0.         0.09046537 0.05006797 0.         0.02365715 0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.13598846 0.         0.05468839 0.04727116 0.         0.\n",
      "   0.         0.03248655 0.05415042 0.         0.         0.\n",
      "   0.         0.00435075 0.09720369 0.         0.         0.\n",
      "   0.08369314 0.         0.         0.05793652 0.         0.08219028\n",
      "   0.         0.         0.01177892 0.10881354 0.         0.\n",
      "   0.         0.06474225 0.05809931 0.         0.         0.02664913\n",
      "   0.01288957 0.         0.03107853 0.         0.         0.\n",
      "   0.13248082 0.         0.         0.         0.         0.\n",
      "   0.         0.10522534 0.         0.07495283 0.03101982 0.05283207\n",
      "   0.00466938 0.09514671 0.00054616 0.        ]\n",
      "  [0.         0.         0.06829739 0.09849601 0.11844321 0.\n",
      "   0.10648596 0.         0.         0.06946453 0.         0.00135783\n",
      "   0.04175166 0.         0.         0.10055738 0.10648762 0.\n",
      "   0.         0.         0.         0.07734827 0.         0.\n",
      "   0.         0.07906882 0.         0.         0.         0.02494789\n",
      "   0.         0.09046537 0.05006797 0.         0.02365715 0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.13598846 0.         0.05468839 0.04727116 0.         0.\n",
      "   0.         0.03248655 0.05415042 0.         0.         0.\n",
      "   0.         0.00435075 0.09720369 0.         0.         0.\n",
      "   0.08369314 0.         0.         0.05793652 0.         0.08219028\n",
      "   0.         0.         0.01177892 0.10881354 0.         0.\n",
      "   0.         0.06474225 0.05809931 0.         0.         0.02664913\n",
      "   0.01288957 0.         0.03107853 0.         0.         0.\n",
      "   0.13248082 0.         0.         0.         0.         0.\n",
      "   0.         0.10522534 0.         0.07495283 0.03101982 0.05283207\n",
      "   0.00466938 0.09514671 0.00054616 0.        ]\n",
      "  [0.         0.         0.06829739 0.09849601 0.11844321 0.\n",
      "   0.10648596 0.         0.         0.06946453 0.         0.00135783\n",
      "   0.04175166 0.         0.         0.10055738 0.10648762 0.\n",
      "   0.         0.         0.         0.07734827 0.         0.\n",
      "   0.         0.07906882 0.         0.         0.         0.02494789\n",
      "   0.         0.09046537 0.05006797 0.         0.02365715 0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.13598846 0.         0.05468839 0.04727116 0.         0.\n",
      "   0.         0.03248655 0.05415042 0.         0.         0.\n",
      "   0.         0.00435075 0.09720369 0.         0.         0.\n",
      "   0.08369314 0.         0.         0.05793652 0.         0.08219028\n",
      "   0.         0.         0.01177892 0.10881354 0.         0.\n",
      "   0.         0.06474225 0.05809931 0.         0.         0.02664913\n",
      "   0.01288957 0.         0.03107853 0.         0.         0.\n",
      "   0.13248082 0.         0.         0.         0.         0.\n",
      "   0.         0.10522534 0.         0.07495283 0.03101982 0.05283207\n",
      "   0.00466938 0.09514671 0.00054616 0.        ]\n",
      "  [0.         0.         0.06829739 0.09849601 0.11844321 0.\n",
      "   0.10648596 0.         0.         0.06946453 0.         0.00135783\n",
      "   0.04175166 0.         0.         0.10055738 0.10648762 0.\n",
      "   0.         0.         0.         0.07734827 0.         0.\n",
      "   0.         0.07906882 0.         0.         0.         0.02494789\n",
      "   0.         0.09046537 0.05006797 0.         0.02365715 0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.13598846 0.         0.05468839 0.04727116 0.         0.\n",
      "   0.         0.03248655 0.05415042 0.         0.         0.\n",
      "   0.         0.00435075 0.09720369 0.         0.         0.\n",
      "   0.08369314 0.         0.         0.05793652 0.         0.08219028\n",
      "   0.         0.         0.01177892 0.10881354 0.         0.\n",
      "   0.         0.06474225 0.05809931 0.         0.         0.02664913\n",
      "   0.01288957 0.         0.03107853 0.         0.         0.\n",
      "   0.13248082 0.         0.         0.         0.         0.\n",
      "   0.         0.10522534 0.         0.07495283 0.03101982 0.05283207\n",
      "   0.00466938 0.09514671 0.00054616 0.        ]\n",
      "  [0.         0.         0.06829739 0.09849601 0.11844321 0.\n",
      "   0.10648596 0.         0.         0.06946453 0.         0.00135783\n",
      "   0.04175166 0.         0.         0.10055738 0.10648762 0.\n",
      "   0.         0.         0.         0.07734827 0.         0.\n",
      "   0.         0.07906882 0.         0.         0.         0.02494789\n",
      "   0.         0.09046537 0.05006797 0.         0.02365715 0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.13598846 0.         0.05468839 0.04727116 0.         0.\n",
      "   0.         0.03248655 0.05415042 0.         0.         0.\n",
      "   0.         0.00435075 0.09720369 0.         0.         0.\n",
      "   0.08369314 0.         0.         0.05793652 0.         0.08219028\n",
      "   0.         0.         0.01177892 0.10881354 0.         0.\n",
      "   0.         0.06474225 0.05809931 0.         0.         0.02664913\n",
      "   0.01288957 0.         0.03107853 0.         0.         0.\n",
      "   0.13248082 0.         0.         0.         0.         0.\n",
      "   0.         0.10522534 0.         0.07495283 0.03101982 0.05283207\n",
      "   0.00466938 0.09514671 0.00054616 0.        ]]]\n"
     ]
    }
   ],
   "source": [
    "# lstm autoencoder recreate sequence\n",
    "from numpy import array\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.utils import plot_model\n",
    "\n",
    "sequence = array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])\n",
    "\n",
    "n_in = len(sequence)\n",
    "sequence = sequence.reshape((1, n_in, 1))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, activation='relu', input_shape=(n_in,1)))\n",
    "model.add(RepeatVector(n_in))\n",
    "model.add(LSTM(100, activation='relu', return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(1)))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "# fit model\n",
    "model.fit(sequence, sequence, epochs=300, verbose=0)\n",
    "\n",
    "################################\n",
    "# Output of Layer 1\n",
    "################################\n",
    "model = Model(inputs=model.inputs, outputs=model.layers[1].output)\n",
    "\n",
    "# get the feature vector for the input sequence\n",
    "yhat = model.predict(sequence)\n",
    "print(yhat.shape)\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.3 Layer 03 (LSTM)\n",
    "This is another LSTM layer. We have already setup the 3D input of the this layer using RepeatVector in the previous layer. This layer uses 100 nodes. Hence the output should be of (1, 100) shape. However, we have repeated the input vector using RepeatVector 9 times, hence the output shape from this layer is (1, 9, 100). Example in the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 9, 100)\n",
      "[[[4.64976057e-02 2.06131376e-02 5.08040003e-02 7.50926556e-03\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 2.83644926e-02 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.83519516e-02 1.23489164e-02\n",
      "   2.26073666e-03 0.00000000e+00 2.92143710e-02 2.48436835e-02\n",
      "   3.61590311e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 1.79621559e-02\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 2.65220553e-03\n",
      "   0.00000000e+00 4.07699011e-02 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 3.89819629e-02 4.06789966e-02 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 4.01959606e-02 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 3.23795229e-02 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 3.19472440e-02 0.00000000e+00\n",
      "   4.05337997e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 3.30425389e-02 2.27643806e-03 4.70290929e-02\n",
      "   2.73408908e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   2.88463701e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   3.26847099e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 3.75911258e-02 0.00000000e+00\n",
      "   4.75568175e-02 0.00000000e+00 2.78033447e-02 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 4.43441309e-02 0.00000000e+00]\n",
      "  [8.89841467e-02 3.86100784e-02 9.70720127e-02 1.89746041e-02\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 5.05850725e-02 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 3.79125327e-02 2.13974603e-02\n",
      "   1.12017132e-02 0.00000000e+00 5.55769913e-02 5.21058999e-02\n",
      "   6.75377175e-02 4.66216850e-04 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 3.45330872e-02\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 6.47990825e-03\n",
      "   0.00000000e+00 8.25644061e-02 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 5.69005907e-02 7.77455121e-02 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 8.06689784e-02 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 6.19004332e-02 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 6.04347549e-02 1.74166518e-03\n",
      "   7.60750175e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 6.57759011e-02 2.85830954e-03 9.16512981e-02\n",
      "   5.05791418e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   6.23636171e-02 0.00000000e+00 2.90242420e-03 0.00000000e+00\n",
      "   6.50134310e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 7.44674504e-02 0.00000000e+00\n",
      "   8.87012035e-02 0.00000000e+00 5.24314381e-02 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 8.69983509e-02 0.00000000e+00]\n",
      "  [1.28439873e-01 5.47338985e-02 1.40074506e-01 3.35533805e-02\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 6.83972538e-02 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 5.78732491e-02 2.78558247e-02\n",
      "   2.41338443e-02 3.19417799e-03 7.97332004e-02 8.06998014e-02\n",
      "   9.53288376e-02 3.72619159e-03 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 4.97498959e-02\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 1.09205749e-02\n",
      "   0.00000000e+00 1.24578014e-01 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 6.08592369e-02 1.12411201e-01 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 1.20935909e-01 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 8.92719030e-02 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 8.67330953e-02 8.15997925e-03\n",
      "   1.07804962e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 9.71685052e-02 2.08261097e-03 1.34447485e-01\n",
      "   7.05578402e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   9.82995331e-02 0.00000000e+00 9.53983143e-03 0.00000000e+00\n",
      "   9.73369330e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.10214591e-01 0.00000000e+00\n",
      "   1.25591263e-01 0.00000000e+00 7.47243911e-02 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.28077045e-01 0.00000000e+00]\n",
      "  [1.65620089e-01 6.94896504e-02 1.80642292e-01 5.05314656e-02\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 8.29785317e-02 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 7.77546391e-02 3.23641188e-02\n",
      "   3.93884815e-02 1.18477652e-02 1.02250285e-01 1.10005826e-01\n",
      "   1.20382078e-01 8.51171650e-03 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 6.36821836e-02\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 1.56012159e-02\n",
      "   0.00000000e+00 1.66453943e-01 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 5.58562987e-02 1.45498157e-01 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 1.60775363e-01 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 1.15033522e-01 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.11490235e-01 1.76135991e-02\n",
      "   1.36653647e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 1.26741976e-01 1.41504896e-03 1.75856233e-01\n",
      "   8.80060792e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   1.35429367e-01 0.00000000e+00 1.85900647e-02 0.00000000e+00\n",
      "   1.29905865e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.44596383e-01 0.00000000e+00\n",
      "   1.59693420e-01 0.00000000e+00 9.51901227e-02 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.67798728e-01 0.00000000e+00]\n",
      "  [2.01184422e-01 8.31407458e-02 2.19345301e-01 6.93398267e-02\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 9.51348916e-02 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 9.72496346e-02 3.54116186e-02\n",
      "   5.59604578e-02 2.40190029e-02 1.23579137e-01 1.39635369e-01\n",
      "   1.43309221e-01 1.40774632e-02 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 7.64343515e-02\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 2.03354992e-02\n",
      "   0.00000000e+00 2.07992673e-01 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 4.53075729e-02 1.77639440e-01 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 2.00110018e-01 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 1.39534190e-01 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.35140806e-01 2.89434791e-02\n",
      "   1.63351685e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 1.54389963e-01 9.59906843e-04 2.16214284e-01\n",
      "   1.03520915e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   1.73167899e-01 0.00000000e+00 2.90801264e-02 0.00000000e+00\n",
      "   1.62837192e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.77523151e-01 0.00000000e+00\n",
      "   1.91964328e-01 0.00000000e+00 1.14219159e-01 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 2.06481606e-01 0.00000000e+00]\n",
      "  [2.35672355e-01 9.58716571e-02 2.56689459e-01 8.95429105e-02\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 1.05450377e-01 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.16238505e-01 3.74003276e-02\n",
      "   7.32206106e-02 3.83830294e-02 1.44048974e-01 1.69361770e-01\n",
      "   1.64588302e-01 1.99981146e-02 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 8.81552696e-02\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 2.50311587e-02\n",
      "   0.00000000e+00 2.49154106e-01 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 3.15526612e-02 2.09288821e-01 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 2.39030913e-01 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 1.63066313e-01 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.58053279e-01 4.13866490e-02\n",
      "   1.88490659e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 1.80191875e-01 6.50136033e-04 2.55867124e-01\n",
      "   1.17564805e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   2.11288050e-01 0.00000000e+00 4.03714254e-02 0.00000000e+00\n",
      "   1.96166411e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 2.09035859e-01 0.00000000e+00\n",
      "   2.23090962e-01 0.00000000e+00 1.32127851e-01 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 2.44424030e-01 0.00000000e+00]\n",
      "  [2.69537449e-01 1.07828490e-01 2.93131858e-01 1.10816956e-01\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 1.14356816e-01 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.34722248e-01 3.86423506e-02\n",
      "   9.07893926e-02 5.40573262e-02 1.63910627e-01 1.99072719e-01\n",
      "   1.84599534e-01 2.60435119e-02 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 9.90142003e-02\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 2.96521131e-02\n",
      "   0.00000000e+00 2.90012479e-01 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 1.96325425e-02 2.40788817e-01 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 2.77734965e-01 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 1.85890988e-01 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.80545270e-01 5.44577055e-02\n",
      "   2.12550014e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 2.04335719e-01 4.39662923e-04 2.95157701e-01\n",
      "   1.30498856e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   2.49781951e-01 0.00000000e+00 5.20617105e-02 0.00000000e+00\n",
      "   2.29910195e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 2.39251778e-01 0.00000000e+00\n",
      "   2.53586888e-01 0.00000000e+00 1.49174407e-01 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 2.81927317e-01 0.00000000e+00]\n",
      "  [3.03196669e-01 1.19174793e-01 3.29175383e-01 1.32880494e-01\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 1.22134186e-01 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.52927458e-01 3.93338203e-02\n",
      "   1.08465143e-01 7.04676658e-02 1.83395907e-01 2.28776589e-01\n",
      "   2.03663290e-01 3.21440138e-02 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 1.09232217e-01\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 3.42006981e-02\n",
      "   0.00000000e+00 3.30749542e-01 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 1.21063786e-02 2.72433788e-01 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 3.16612899e-01 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 2.08293915e-01 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 2.02889800e-01 6.78923875e-02\n",
      "   2.35959485e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 2.27190495e-01 2.96933344e-04 3.34485799e-01\n",
      "   1.42656058e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   2.88967341e-01 0.00000000e+00 6.39319941e-02 0.00000000e+00\n",
      "   2.64079422e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 2.68339306e-01 0.00000000e+00\n",
      "   2.83847451e-01 0.00000000e+00 1.65557563e-01 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 3.19375753e-01 0.00000000e+00]\n",
      "  [3.37057114e-01 1.30081311e-01 3.65325004e-01 1.55529067e-01\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 1.28983855e-01 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 1.71180099e-01 3.96001898e-02\n",
      "   1.26162931e-01 8.72700810e-02 2.02722132e-01 2.58554190e-01\n",
      "   2.22069606e-01 3.83103304e-02 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 1.19036324e-01\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 3.86981182e-02\n",
      "   0.00000000e+00 3.71631831e-01 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 7.39710685e-03 3.04509014e-01 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 3.56155187e-01 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 2.30565116e-01 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 2.25348681e-01 8.15734789e-02\n",
      "   2.59100527e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 2.49188349e-01 2.00303024e-04 3.74298126e-01\n",
      "   1.54329285e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   3.29316080e-01 0.00000000e+00 7.58833960e-02 0.00000000e+00\n",
      "   2.98729718e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 2.96514153e-01 0.00000000e+00\n",
      "   3.14202696e-01 0.00000000e+00 1.81454912e-01 0.00000000e+00\n",
      "   0.00000000e+00 0.00000000e+00 3.57190162e-01 0.00000000e+00]]]\n"
     ]
    }
   ],
   "source": [
    "# lstm autoencoder recreate sequence\n",
    "from numpy import array\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.utils import plot_model\n",
    "\n",
    "sequence = array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])\n",
    "\n",
    "n_in = len(sequence)\n",
    "sequence = sequence.reshape((1, n_in, 1))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, activation='relu', input_shape=(n_in,1)))\n",
    "model.add(RepeatVector(n_in))\n",
    "model.add(LSTM(100, activation='relu', return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(1)))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "# fit model\n",
    "model.fit(sequence, sequence, epochs=300, verbose=0)\n",
    "\n",
    "################################\n",
    "# Output of Third Layer \n",
    "################################\n",
    "model = Model(inputs=model.inputs, outputs=model.layers[2].output)\n",
    "\n",
    "# get the feature vector for the input sequence\n",
    "yhat = model.predict(sequence)\n",
    "print(yhat.shape)\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.4 Layer 4 (Last Layer, i.e, TimeDistributed)\n",
    "Here this layer comprises of only 1 node, `Dense(1)`, ie., every node output from the last layer is connected to this single node as input. And, we had 9 repeated vector, the node predicts a single number for each of the vector generating 9 sequences. Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 9, 1)\n",
      "[[[0.10291987]\n",
      "  [0.19771233]\n",
      "  [0.2986349 ]\n",
      "  [0.4012162 ]\n",
      "  [0.5014176 ]\n",
      "  [0.60020894]\n",
      "  [0.69911045]\n",
      "  [0.79914594]\n",
      "  [0.900632  ]]]\n"
     ]
    }
   ],
   "source": [
    "# lstm autoencoder recreate sequence\n",
    "from numpy import array\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.utils import plot_model\n",
    "\n",
    "sequence = array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])\n",
    "\n",
    "n_in = len(sequence)\n",
    "sequence = sequence.reshape((1, n_in, 1))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, activation='relu', input_shape=(n_in,1)))\n",
    "model.add(RepeatVector(n_in))\n",
    "model.add(LSTM(100, activation='relu', return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(1)))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "# fit model\n",
    "model.fit(sequence, sequence, epochs=300, verbose=0)\n",
    "\n",
    "################################\n",
    "# Output of Fourth Layer\n",
    "################################\n",
    "model = Model(inputs=model.inputs, outputs=model.layers[3].output)\n",
    "\n",
    "# get the feature vector for the input sequence\n",
    "yhat = model.predict(sequence)\n",
    "print(yhat.shape)\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Reproduce the input Sequence.\n",
    "The full code for reproducing the input sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 9, 1)\n",
      "[[[0.10714255]\n",
      "  [0.20551506]\n",
      "  [0.30306545]\n",
      "  [0.40028146]\n",
      "  [0.49773994]\n",
      "  [0.596095  ]\n",
      "  [0.6960797 ]\n",
      "  [0.7986292 ]\n",
      "  [0.90475166]]]\n"
     ]
    }
   ],
   "source": [
    "# lstm autoencoder recreate sequence\n",
    "from numpy import array\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.utils import plot_model\n",
    "\n",
    "sequence = array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])\n",
    "\n",
    "n_in = len(sequence)\n",
    "sequence = sequence.reshape((1, n_in, 1))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, activation='relu', input_shape=(n_in,1)))\n",
    "model.add(RepeatVector(n_in))\n",
    "model.add(LSTM(100, activation='relu', return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(1)))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "# fit model\n",
    "model.fit(sequence, sequence, epochs=300, verbose=0)\n",
    "\n",
    "\n",
    "# get the feature vector for the input sequence\n",
    "yhat = model.predict(sequence)\n",
    "print(yhat.shape)\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0 Keeping the Standalone Encoder\n",
    "As we have discussed, we can use the output of first layer, LSTM as the encoder of the input. That is we train and fit the whole model and at the end we just use the ouptut of first LSTM layer as encoder. An example for such encoder is in the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 100)\n",
      "[[0.         0.         0.05114636 0.         0.03163566 0.\n",
      "  0.10090625 0.08232216 0.         0.06240095 0.         0.\n",
      "  0.13956766 0.         0.02350081 0.         0.0130856  0.\n",
      "  0.0372422  0.         0.         0.05535569 0.06782397 0.\n",
      "  0.         0.1143057  0.         0.         0.08879374 0.07698841\n",
      "  0.         0.         0.04171373 0.         0.         0.03945698\n",
      "  0.09528719 0.         0.06141445 0.10190405 0.         0.\n",
      "  0.01744407 0.         0.         0.02750597 0.05886816 0.\n",
      "  0.06366922 0.         0.07811875 0.         0.09518047 0.\n",
      "  0.11582238 0.         0.00261876 0.         0.05623443 0.08539794\n",
      "  0.07995928 0.03876071 0.12081884 0.         0.         0.04304748\n",
      "  0.         0.05891561 0.09684899 0.         0.04247667 0.05588685\n",
      "  0.         0.04837837 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.06170175 0.11355913 0.\n",
      "  0.10680667 0.         0.06447414 0.0725139  0.         0.0655063\n",
      "  0.07715986 0.         0.         0.06276695 0.         0.03047177\n",
      "  0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# lstm autoencoder recreate sequence\n",
    "from numpy import array\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.utils import plot_model\n",
    "# define input sequence\n",
    "sequence = array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])\n",
    "# reshape input into [samples, timesteps, features]\n",
    "n_in = len(sequence)\n",
    "sequence = sequence.reshape((1, n_in, 1))\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, activation='relu', input_shape=(n_in,1)))\n",
    "model.add(RepeatVector(n_in))\n",
    "model.add(LSTM(100, activation='relu', return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(1)))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "# fit model\n",
    "model.fit(sequence, sequence, epochs=300, verbose=0)\n",
    "# connect the encoder LSTM as the output layer\n",
    "model = Model(inputs=model.inputs, outputs=model.layers[0].output)\n",
    "\n",
    "# get the feature vector for the input sequence\n",
    "yhat = model.predict(sequence)\n",
    "print(yhat.shape)\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
